{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Introduction </h3>\n",
    "\n",
    "In this post we will understand and implement the components of the modern object detection model Faster-RCNN. Object detectors involve many parts and\n",
    "it can be difficult to follow the code in the open implementations available. Here we will give a clear layout of of such a model. This post is divided into two parts. In this first one, we will construct the Faster-RCNN network and several of it's components. This will allow us to perform inference using the pretrained weights available from <a href=\"https://github.com/facebookresearch/Detectron/blob/master/MODEL_ZOO.md\">Facebook's Detectron</a>, which will help us understand how that object-detection framework works. In the  second part of the post, we will actually train the network ourselves and understand the steps involved in that process. \n",
    "\n",
    "Here we will take certain components for granted - NMS, bounding box transformations, ROI-Align, and the proposal layer. For an explanation of how these work please consult the papers and the full code repo. The focus here is more on how the pieces fit together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> The Two Stages </h3>\n",
    "\n",
    "The Faster-RCNN family of detectors works in two stages. The first stage, the Region Proposal Network (RPN), outputs box regions and their associated 'objectness' score (i.e., object vs no object). These proposals are filtered, then used to crop features from the top-level of the backbone feature extractor (e.g., Resnet-50). This process of feature cropping was done by ROI-pooling in the original Faster-RCNN, or more recently, using ROI-Align in Mask-RCNN. The second stage, the Faster-RCNN network, takes these cropped features and refines the initial proposals of RPN, along with predicting the probability of the object class. The figure below from the Faster-RCNN paper illustrates this two-stage process:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "In the Faster-RCNN version with the Resnet feature extractor (or backbone), RPN operates on the final convolutional layer of the C4 block [3]. ROI-align is performed on the C4 features and those pooled features are first fed through the C5 block and the average pool of Resnet. These last two operations replace the fully-connected layers of VGG in the original application of Resnet to object-detection [3]. After that, the Faster-RCNN network predicts bounding-boxes and classes.\n",
    "\n",
    "The code below show the structure of this computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, rpn, backbone, roi_align, n_classes):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        self.backbone = backbone  # eg. Resnet-50\n",
    "        self.rpn = rpn  # Region Proposal Network\n",
    "        self.roi_align = roi_align  # ROI-Align layer\n",
    "        self.bbox_pred = nn.Linear(2048, 4 * n_classes)  # bounding-box head\n",
    "        self.cls_score = nn.Linear(2048, n_classes)  # class logits head\n",
    "\n",
    "    def forward(self, images, h, w, im_scale):\n",
    "        features =elf.backbone(images)  # compute block C4 features for image\n",
    "        proposals, scores = self.rpn(features, h, w, im_scale)  # apply RPN to get proposals\n",
    "        pooled_feat = self.roi_align(features, proposals)  # apply ROI align on the C4 features using the proposals\n",
    "        pooled_feat = self.backbone.top(pooled_feat).squeeze()  # apply the C5 block and Average pool of Resnet\n",
    "        bbox_pred = self.bbox_pred(pooled_feat)  # apply bounding-box head\n",
    "        cls_score = self.cls_score(pooled_feat)  # apply class-score head\n",
    "        cls_prob = F.softmax(cls_score, dim=1)  # softmax to get object-class probabilities\n",
    "\n",
    "        return {'bbox_pred': bbox_pred, 'cls_prob': cls_prob, 'rois': proposals}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the C5 block and the Average Pool of Resnet produce a 2048-dimensional vector per ROI. The bounding-box heads and class-score heads are fully-connected layers that operate on those ROI vectors. The class-score head output dimension is the number of object classes. The bounding-box head output dimension is 4 times the number of classes. This is because a bounding box is predicted separately for each object class, later we take the box corresponding to the class with the highest score.\n",
    "\n",
    "Now that we see the overal structure of the networks, let's see how each part works in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> The Region Proposal Network </h3>\n",
    "\n",
    "RPN works on the top convolutional layer of the C4 block. First it applies a convolution with a 3x3 kernel to get another feature map with 1024 output channels. Then two convolutional layers with 1x1 kernels are applied to get proposals and scores <b>per cell</b> in the feature map, as shown in the figure below from the paper [1]:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "The output channels for the scores convolutional head give the 'objectness' score for each anchor at that cell. The output channels for the proposal convolutional head give the bounding boxes for each anchor at that cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_anchors, proposal_layer):\n",
    "        super(RPN, self).__init__()\n",
    "\n",
    "        self.n_anchors = n_anchors\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.bbox_pred = nn.Conv2d(out_channels, self.n_anchors * 4, 1)\n",
    "        self.cls_score = nn.Conv2d(out_channels, self.n_anchors, 1)\n",
    "        self.proposal_layer = proposal_layer\n",
    "\n",
    "    def forward(self, feature_map, h, w, im_scale):\n",
    "        x = self.conv(feature_map)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        cls_score = self.cls_score(x)\n",
    "        cls_prob = F.sigmoid(cls_score)\n",
    "        bbox_pred = self.bbox_pred(x)\n",
    "        proposals, scores = self.proposal_layer(cls_prob, bbox_pred,\n",
    "                                                h, w, im_scale)\n",
    "        return proposals, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note at the end of the forward method we make a call to a proposal layer. This layer performs additional computations and filtering on the given proposals before they are passed further on. This because the outputs of the RPN need to be applied to the anchors at every cell to get the actual proposal, and the proposals need to be filtered to make sense and reduce their huge number. The steps it does are:\n",
    "\n",
    "<ol> <li> for each location on the feature map grid grid: \n",
    "    <ul> <li> generate the anchor boxes centered on cell i </li>\n",
    "        <li> apply predicted bbox deltas to each of the A anchors at cell i </li>\n",
    "    </ul> </li>\n",
    "    <li> clip predicted boxes to image </li>\n",
    "    <li> remove predicted boxes that are smaller than a threshold </li>\n",
    "    <li> sort all proposals by score from highest to lowest </li> \n",
    "    <li> take the top proposals before NMS </li>\n",
    "    <li> apply NMS with a loose threshold (0.7) to the remaining proposals </li>\n",
    "    <li> take top proposals after NMS </li>\n",
    "    <li> return the top proposals </li>\n",
    "</ol>\n",
    "    \n",
    "The code for this is straightforward but a bit involved, so we leave it out of this post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> A Complete Example </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# define some configurations: ################\n",
    "\n",
    "# Number of top scoring boxes to keep before apply NMS to RPN proposals\n",
    "RPN_PRE_NMS_TOP_N = 6000\n",
    "# Number of top scoring boxes to keep after applying NMS to RPN proposals\n",
    "RPN_POST_NMS_TOP_N = 1000\n",
    "# NMS threshold used on RPN proposals\n",
    "RPN_NMS_THRESH = 0.7\n",
    "# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n",
    "RPN_MIN_SIZE = 0\n",
    "# Size of the pooled region after RoI pooling\n",
    "POOLING_SIZE = 14\n",
    "\n",
    "TEST_NMS = 0.5\n",
    "TEST_MAX_SIZE = 1333\n",
    "PIXEL_MEANS = np.array([122.7717, 115.9465, 102.9801])\n",
    "\n",
    "anchor_sizes, anchor_ratios = [32, 64, 128, 256, 512], [0.5, 1, 2]\n",
    "feat_stride = 16\n",
    "\n",
    "############ prepare an image #################\n",
    "\n",
    "x = cv2.imread('samples/15673749081_767a7fa63a_k.jpg')[:, :, ::-1]\n",
    "\n",
    "blobs, im_scales = prep_im_for_blob(x, PIXEL_MEANS, target_sizes=(800,), max_size=TEST_MAX_SIZE)\n",
    "blobs = im_list_to_blob(blobs)\n",
    "img = Variable(torch.from_numpy(blobs))\n",
    "\n",
    "im_info = torch.from_numpy(np.array([[blobs.shape[2], blobs.shape[3], im_scales[0]]], dtype=np.float32))\n",
    "im_size, im_scale = [blobs.shape[2], blobs.shape[3]], im_scales[0]\n",
    "\n",
    "########## make the modules ####################\n",
    "\n",
    "backbone = BackBone()\n",
    "proposal_layer = ProposalLayer(feat_stride, anchor_sizes, anchor_ratios,\n",
    "                               RPN_PRE_NMS_TOP_N, RPN_POST_NMS_TOP_N, RPN_NMS_THRESH, RPN_MIN_SIZE)\n",
    "roi_align = RoIAlign(POOLING_SIZE, POOLING_SIZE, spatial_scale=1./16.)\n",
    "\n",
    "rpn = RPN(1024, 1024, 15, proposal_layer)\n",
    "frcnn = FasterRCNN(rpn, backbone, roi_align, 81)\n",
    "frcnn.load_pretrained_weights('model_final.pkl', 'resnet50_mapping.npy')\n",
    "\n",
    "############ feedforward pass with postprocessing ################\n",
    "\n",
    "frcnn = frcnn.cuda()\n",
    "frcnn.eval()\n",
    "img = img.cuda()\n",
    "\n",
    "output = frcnn(img, im_size[0], im_size[1], im_scale)\n",
    "class_scores, bbox_deltas, rois = output['cls_prob'], output['bbox_pred'], output['rois']\n",
    "\n",
    "scores_final, boxes_final, boxes_per_class = postprocess_output(rois, im_scale, im_size, class_scores, bbox_deltas,\n",
    "                                                                bbox_reg_weights=(10.0, 10.0, 5.0, 5.0))\n",
    "\n",
    "########## visualize ###############\n",
    "\n",
    "vis.vis_one_image(\n",
    "    x,  # BGR -> RGB for visualization\n",
    "    'output',\n",
    "    'samples/',\n",
    "    boxes_per_class,\n",
    "    dataset=None,\n",
    "    box_alpha=0.3,\n",
    "    show_class=True,\n",
    "    thresh=0.7,\n",
    "    ext='jpg'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> References </h3>\n",
    "\n",
    "<ol> \n",
    "     <li> Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems (pp. 91-99). </li>\n",
    "     <li>He, K., Gkioxari, G., Doll√°r, P., & Girshick, R. (2017, October). Mask r-cnn. In Computer Vision (ICCV), 2017 IEEE International Conference on (pp. 2980-2988). IEEE.</li>\n",
    "         <li>  He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). </li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
